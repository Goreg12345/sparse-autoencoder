{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-02T23:11:55.128691200Z",
     "start_time": "2023-11-02T23:11:30.973143400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/var/local/glang/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-small-tokenized-2b-464a33e6a116844b/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"NeelNanda/pile-small-tokenized-2b\", split=\"train\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T23:11:58.302654100Z",
     "start_time": "2023-11-02T23:11:55.130927500Z"
    }
   },
   "id": "9201bdd1f64fb26b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T23:12:03.607748600Z",
     "start_time": "2023-11-02T23:11:58.303693400Z"
    }
   },
   "id": "f3946aa029d4ff71"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'actv_size': 768,\n",
    "    'buffer_batches': 10,\n",
    "    'extraction_batch_size': 8,\n",
    "    'actv_name': 'blocks.8.hook_resid_post',\n",
    "    'layer': 8,\n",
    "    'batch_size': 8,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T23:12:03.622268Z",
     "start_time": "2023-11-02T23:12:03.610145200Z"
    }
   },
   "id": "548819fec278eadf"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": "HookedTransformer(\n  (embed): Embed()\n  (hook_embed): HookPoint()\n  (pos_embed): PosEmbed()\n  (hook_pos_embed): HookPoint()\n  (blocks): ModuleList(\n    (0-11): 12 x TransformerBlock(\n      (ln1): LayerNormPre(\n        (hook_scale): HookPoint()\n        (hook_normalized): HookPoint()\n      )\n      (ln2): LayerNormPre(\n        (hook_scale): HookPoint()\n        (hook_normalized): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_pattern): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n      )\n      (hook_q_input): HookPoint()\n      (hook_k_input): HookPoint()\n      (hook_v_input): HookPoint()\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n  )\n  (ln_final): LayerNormPre(\n    (hook_scale): HookPoint()\n    (hook_normalized): HookPoint()\n  )\n  (unembed): Unembed()\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "llm = HookedTransformer.from_pretrained(\n",
    "    model_name='gpt2-small',\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device='cuda'  # will be moved to GPU later by lightning\n",
    ")\n",
    "llm.requires_grad_(False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T23:12:26.724805200Z",
     "start_time": "2023-11-02T23:12:03.618270600Z"
    }
   },
   "id": "3685b5c55b060b86"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up now\n",
      "refreshing buffer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maimport\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneel_buffer\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mneel_buffer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Buffer\n\u001B[0;32m----> 4\u001B[0m buffer \u001B[38;5;241m=\u001B[39m \u001B[43mBuffer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcfg\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/tmp/pycharm_project_583/neel_buffer.py:26\u001B[0m, in \u001B[0;36mBuffer.__init__\u001B[0;34m(self, llm, dataset, actv_size, buffer_batches, extraction_batch_size, actv_name, layer, batch_size)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset \u001B[38;5;241m=\u001B[39m dataset\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msetting up now\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrefresh\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/tmp/pycharm_project_583/neel_buffer.py:41\u001B[0m, in \u001B[0;36mBuffer.refresh\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     38\u001B[0m all_tokens_reshaped \u001B[38;5;241m=\u001B[39m einops\u001B[38;5;241m.\u001B[39mrearrange(tokens, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch (x seq_len) -> (batch x) seq_len\u001B[39m\u001B[38;5;124m\"\u001B[39m, x\u001B[38;5;241m=\u001B[39mx,\n\u001B[1;32m     39\u001B[0m                                        seq_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m)\n\u001B[1;32m     40\u001B[0m all_tokens_reshaped[:, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mbos_token_id\n\u001B[0;32m---> 41\u001B[0m _, cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_with_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_tokens_reshaped\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_at_layer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;66;03m# names_filter=self.actv_name)\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mprint\u001B[39m(cache\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[1;32m     43\u001B[0m acts \u001B[38;5;241m=\u001B[39m cache[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactv_name]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactv_size)\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:394\u001B[0m, in \u001B[0;36mHookedTransformer.run_with_cache\u001B[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_with_cache\u001B[39m(\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mmodel_args, return_cache_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, remove_batch_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    382\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    389\u001B[0m     Union[ActivationCache, Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor]],\n\u001B[1;32m    390\u001B[0m ]:\n\u001B[1;32m    391\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;124;03m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a dictionary of activations as in HookedRootModule.\u001B[39;00m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 394\u001B[0m     out, cache_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremove_batch_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremove_batch_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_cache_object:\n\u001B[1;32m    398\u001B[0m         cache \u001B[38;5;241m=\u001B[39m ActivationCache(\n\u001B[1;32m    399\u001B[0m             cache_dict, \u001B[38;5;28mself\u001B[39m, has_batch_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m remove_batch_dim\n\u001B[1;32m    400\u001B[0m         )\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/transformer_lens/hook_points.py:363\u001B[0m, in \u001B[0;36mHookedRootModule.run_with_cache\u001B[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001B[0m\n\u001B[1;32m    358\u001B[0m cache_dict, fwd, bwd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_caching_hooks(\n\u001B[1;32m    359\u001B[0m     names_filter, incl_bwd, device, remove_batch_dim\u001B[38;5;241m=\u001B[39mremove_batch_dim\n\u001B[1;32m    360\u001B[0m )\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhooks(fwd_hooks\u001B[38;5;241m=\u001B[39mfwd, bwd_hooks\u001B[38;5;241m=\u001B[39mbwd, reset_hooks_end\u001B[38;5;241m=\u001B[39mreset_hooks_end, clear_contexts\u001B[38;5;241m=\u001B[39mclear_contexts):\n\u001B[0;32m--> 363\u001B[0m     model_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    364\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m incl_bwd:\n\u001B[1;32m    365\u001B[0m         model_out\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:325\u001B[0m, in \u001B[0;36mHookedTransformer.forward\u001B[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001B[0m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m shortformer_pos_embed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    321\u001B[0m         shortformer_pos_embed \u001B[38;5;241m=\u001B[39m shortformer_pos_embed\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m    322\u001B[0m             devices\u001B[38;5;241m.\u001B[39mget_device_for_block_index(i, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg)\n\u001B[1;32m    323\u001B[0m         )\n\u001B[0;32m--> 325\u001B[0m     residual \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresidual\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_kv_cache_entry\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_kv_cache\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpast_kv_cache\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    329\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001B[39;49;00m\n\u001B[1;32m    330\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshortformer_pos_embed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshortformer_pos_embed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# [batch, pos, d_model]\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stop_at_layer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001B[39;00m\n\u001B[1;32m    335\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/transformer_lens/components.py:765\u001B[0m, in \u001B[0;36mTransformerBlock.forward\u001B[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001B[0m\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m shortformer_pos_embed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    759\u001B[0m         shortformer_pos_embed \u001B[38;5;241m=\u001B[39m add_head_dimension(shortformer_pos_embed)\n\u001B[1;32m    761\u001B[0m attn_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhook_attn_out(\n\u001B[1;32m    762\u001B[0m     \u001B[38;5;66;03m# hook the residual stream states that are used to calculate the \u001B[39;00m\n\u001B[1;32m    763\u001B[0m     \u001B[38;5;66;03m# queries, keys and values, independently. \u001B[39;00m\n\u001B[1;32m    764\u001B[0m     \u001B[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001B[39;00m\n\u001B[0;32m--> 765\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    766\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_input\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mln1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_input\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mshortformer_pos_embed\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mshortformer_pos_embed\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    767\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_input\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mln1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey_input\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mshortformer_pos_embed\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mshortformer_pos_embed\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    768\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalue_input\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mln1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue_input\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_kv_cache_entry\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_kv_cache_entry\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    771\u001B[0m )  \u001B[38;5;66;03m# [batch, pos, d_model]\u001B[39;00m\n\u001B[1;32m    772\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39mattn_only \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39mparallel_attn_mlp:\n\u001B[1;32m    773\u001B[0m     resid_mid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhook_resid_mid(\n\u001B[1;32m    774\u001B[0m         resid_pre \u001B[38;5;241m+\u001B[39m attn_out\n\u001B[1;32m    775\u001B[0m     )  \u001B[38;5;66;03m# [batch, pos, d_model]\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/transformer_lens/components.py:416\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry)\u001B[0m\n\u001B[1;32m    411\u001B[0m attn_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhook_attn_scores(attn_scores)\n\u001B[1;32m    412\u001B[0m pattern \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhook_pattern(\n\u001B[1;32m    413\u001B[0m     F\u001B[38;5;241m.\u001B[39msoftmax(attn_scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    414\u001B[0m )  \u001B[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001B[39;00m\n\u001B[1;32m    415\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhook_z(\n\u001B[0;32m--> 416\u001B[0m     \u001B[43meinsum\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch key_pos head_index d_head, \u001B[39;49m\u001B[38;5;130;43;01m\\\u001B[39;49;00m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;124;43m        batch head_index query_pos key_pos -> \u001B[39;49m\u001B[38;5;130;43;01m\\\u001B[39;49;00m\n\u001B[1;32m    419\u001B[0m \u001B[38;5;124;43m        batch query_pos head_index d_head\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    420\u001B[0m \u001B[43m        \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    421\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    422\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    423\u001B[0m )  \u001B[38;5;66;03m# [batch, pos, head_index, d_head]\u001B[39;00m\n\u001B[1;32m    424\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39muse_attn_result:\n\u001B[1;32m    425\u001B[0m     out \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    426\u001B[0m         (\n\u001B[1;32m    427\u001B[0m             einsum(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    435\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb_O\n\u001B[1;32m    436\u001B[0m     )  \u001B[38;5;66;03m# [batch, pos, d_model]\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/fancy_einsum/__init__.py:136\u001B[0m, in \u001B[0;36meinsum\u001B[0;34m(equation, *operands)\u001B[0m\n\u001B[1;32m    134\u001B[0m backend \u001B[38;5;241m=\u001B[39m get_backend(operands[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    135\u001B[0m new_equation \u001B[38;5;241m=\u001B[39m convert_equation(equation)\n\u001B[0;32m--> 136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbackend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meinsum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_equation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moperands\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/fancy_einsum/__init__.py:54\u001B[0m, in \u001B[0;36mTorchBackend.einsum\u001B[0;34m(self, equation, *operands)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21meinsum\u001B[39m(\u001B[38;5;28mself\u001B[39m, equation, \u001B[38;5;241m*\u001B[39moperands):\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meinsum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mequation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moperands\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/torch/functional.py:378\u001B[0m, in \u001B[0;36meinsum\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m einsum(equation, \u001B[38;5;241m*\u001B[39m_operands)\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(operands) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m opt_einsum\u001B[38;5;241m.\u001B[39menabled:\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001B[39;00m\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;66;03m# or the user has disabled using opt_einsum\u001B[39;00m\n\u001B[0;32m--> 378\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meinsum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mequation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperands\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    380\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m opt_einsum\u001B[38;5;241m.\u001B[39mis_available():\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "%aimport neel_buffer\n",
    "from neel_buffer import Buffer\n",
    "\n",
    "buffer = Buffer(\n",
    "    llm,\n",
    "    dataset,\n",
    "    **cfg\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T23:13:22.406145200Z",
     "start_time": "2023-11-02T23:12:26.724805200Z"
    }
   },
   "id": "60545f303d9bd3e2"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# cuda clear memory\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/serimats/lib/python3.10/site-packages/torch/cuda/memory.py:133\u001B[0m, in \u001B[0;36mempty_cache\u001B[0;34m()\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;124;03m`nvidia-smi`.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03m    more details about GPU memory management.\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[0;32m--> 133\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_emptyCache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# cuda clear memory\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T23:14:27.951051800Z",
     "start_time": "2023-11-02T23:14:27.840127300Z"
    }
   },
   "id": "e5e4ab8ce6dea990"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0.)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T23:14:30.940351600Z",
     "start_time": "2023-11-02T23:14:30.895239600Z"
    }
   },
   "id": "21c36eb4d6e12af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bdc59ad6ce2d0eda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ee8f18b02733bcaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c346204b1880fc87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
