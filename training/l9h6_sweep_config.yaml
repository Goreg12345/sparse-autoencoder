program: training/wandb_sweep.py
name: q-l9h6_sweep
method: random
metric:
  name: val_combined_loss
  goal: maximize
parameters:
  lr:
    values:
      - 0.0001
      - 0.001
      - 0.01
  beta1:
    values:
      - 0
      - 0.9
  beta2:
    values:
      - 0.
      - 0.999
      - 0.9999
  batch_size:
    values:
      - 128
      - 256
      - 512
      - 1024
      - 2048
      - 8192
      - 32768
  d_hidden:
    values:
      - 2
      - 32
      - 128
      - 256
      - 512
      - 1024
      - 2048
      - 8192
      - 32768
  l1_coefficient:
    min: 0.004
    max: 0.07
    distribution: uniform
  l1_exponent:
    value: 1
  train_steps:
    value: 100000
  resampling_steps:
    values:
      - [50000]
      - []
  lr_warmup_steps:
    values:
      - 1
      - 3000
  actv_size:
    value: 64
  buffer_size:
    value: 1e7
  actv_name:
    value: blocks.9.attn.hook_q
  layer:
    value: 9
  seq_len:
    value: 512
  dataset_name:
    value: Skylion007/openwebtext
  language_model:
    value: gpt2-small
  use_disc_dataset:
    value: true
  store_path:
    value: /var/local/glang/activations
  store_accessor:
    value: tensor
  store_size:
    value: 250000000
  standardize_activations:
    value: true
  init_geometric_median:
    values:
      - true
      - false
  n_resampling_watch_steps:
    value: 5000
  head:
    value: 6
  reconstruction_loss_batch_size:
    value: 16
  n_resampler_samples:
    value: 819200
  wandb_name:
    value: ''
  ckpt_name:
    value: ''
  extraction_batch_size:
    value: 100
  adjust_for_dict_size:
    value: false
  allow_lower_decoder_norm:
    value: true
  disable_decoder_bias:
    values:
      - true
      - false
  start_lr_decay:
    values:
      - 80000
      - 10000000
  end_lr_decay:
    value: 100000
